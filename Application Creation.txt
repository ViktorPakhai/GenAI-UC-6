Model: GPT-4

User
Write AWS lambda python code that collects the metrics from the number and overall size of unattached disk volumes, not encrypted disk volumes, and not encrypted snapshots. Store report to s3 bucket in json format, report has a data in report name. Use envs for s3 bucket name and report name.
ChatGPT
Here's an example of a Python script that you can run on AWS Lambda. It uses the Boto3 library to interact with AWS services.

This script collects metrics on unattached EBS volumes and non-encrypted EBS volumes, as well as non-encrypted EBS snapshots, then stores a JSON report in an S3 bucket.

python
Copy code
import os
import json
import boto3
from datetime import datetime

def lambda_handler(event, context):

    ec2 = boto3.client('ec2')
    s3 = boto3.client('s3')

    bucket_name = os.environ['BUCKET_NAME']
    report_name = os.environ['REPORT_NAME'] + '_' + datetime.now().strftime("%Y%m%d%H%M%S") + '.json'

    unattached_volumes = []
    unencrypted_volumes = []
    unencrypted_snapshots = []

    # Get all volumes
    response_volumes = ec2.describe_volumes()

    for volume in response_volumes['Volumes']:
        # Check if volume is unattached
        if len(volume['Attachments']) == 0:
            unattached_volumes.append(volume)

        # Check if volume is unencrypted
        if not volume['Encrypted']:
            unencrypted_volumes.append(volume)

    # Get all snapshots
    response_snapshots = ec2.describe_snapshots(OwnerIds=['self'])

    for snapshot in response_snapshots['Snapshots']:
        # Check if snapshot is unencrypted
        if not snapshot['Encrypted']:
            unencrypted_snapshots.append(snapshot)

    # Create a report
    report = {
        'UnattachedVolumes': unattached_volumes,
        'UnencryptedVolumes': unencrypted_volumes,
        'UnencryptedSnapshots': unencrypted_snapshots
    }

    # Write the report to a JSON file
    report_json = json.dumps(report, default=str)

    # Save the report to S3
    s3.put_object(Body=report_json, Bucket=bucket_name, Key=report_name)
Remember to replace BUCKET_NAME and REPORT_NAME with your own environmental variable names. Also, make sure that your Lambda function has the necessary IAM permissions to read EBS volumes and snapshots and to write to your S3 bucket.

This script currently collects all the information related to each volume and snapshot. If you want to restrict this to only specific pieces of information, you can modify the code in the loop where the volumes and snapshots are being appended to the lists.





User
what  permission needs for this lambda to have access to s3 and volumes
ChatGPT
Your AWS Lambda function will need the following permissions:

AmazonEC2ReadOnlyAccess: This allows your function to read information about EBS volumes and EBS snapshots.

AmazonS3FullAccess: This allows your function to write reports to the S3 bucket.

These are managed IAM policies that can be attached to the IAM role that your Lambda function uses. You can do this from the AWS Management Console:

Go to the IAM console.
In the navigation pane, choose Roles.
Choose the role that your Lambda function uses.
Choose Attach policies.
In the list of policies, select the checkbox next to AmazonEC2ReadOnlyAccess and AmazonS3FullAccess.
Choose Attach policy.
Note that the AmazonS3FullAccess policy allows full access to all of your S3 resources. For better security, you can create a custom policy that only allows access to the specific S3 bucket that your function needs to write to. Here's an example of such a policy:

json
Copy code
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::your-bucket-name",
                "arn:aws:s3:::your-bucket-name/*"
            ]
        }
    ]
}
In the policy above, replace your-bucket-name with the name of your S3 bucket. This policy allows your Lambda function to read, write, and list objects in the specified S3 bucket.





User
add AvailabilityZone as env to lambda.
ChatGPT
To collect metrics only from a specific Availability Zone, you could add a filter to the describe_volumes() and describe_snapshots() methods. Here is the updated code:

python
Copy code
import os
import json
import boto3
from datetime import datetime

def lambda_handler(event, context):

    ec2 = boto3.client('ec2')
    s3 = boto3.client('s3')

    bucket_name = os.environ['BUCKET_NAME']
    report_name = os.environ['REPORT_NAME'] + '_' + datetime.now().strftime("%Y%m%d%H%M%S") + '.json'
    availability_zone = os.environ['AVAILABILITY_ZONE']

    unattached_volumes = []
    unencrypted_volumes = []
    unencrypted_snapshots = []

    # Get all volumes in the specified availability zone
    response_volumes = ec2.describe_volumes(Filters=[{'Name': 'availability-zone', 'Values': [availability_zone]}])

    for volume in response_volumes['Volumes']:
        # Check if volume is unattached
        if len(volume['Attachments']) == 0:
            unattached_volumes.append(volume)

        # Check if volume is unencrypted
        if not volume['Encrypted']:
            unencrypted_volumes.append(volume)

    # Get all snapshots in the specified availability zone
    response_snapshots = ec2.describe_snapshots(OwnerIds=['self'], Filters=[{'Name': 'availability-zone', 'Values': [availability_zone]}])

    for snapshot in response_snapshots['Snapshots']:
        # Check if snapshot is unencrypted
        if not snapshot['Encrypted']:
            unencrypted_snapshots.append(snapshot)

    # Create a report
    report = {
        'UnattachedVolumes': unattached_volumes,
        'UnencryptedVolumes': unencrypted_volumes,
        'UnencryptedSnapshots': unencrypted_snapshots
    }

    # Write the report to a JSON file
    report_json = json.dumps(report, default=str)

    # Save the report to S3
    s3.put_object(Body=report_json, Bucket=bucket_name, Key=report_name)
You need to add AVAILABILITY_ZONE as an environment variable in your Lambda function configuration, with the value set to the desired availability zone (for example, "us-east-1a").

Please note that EBS snapshots are not tied to a specific availability zone, so the availability zone filter does not apply to snapshots. This is because once a snapshot is created, it can be used to create new volumes in any availability zone within the same region. Hence, the describe_snapshots() call doesn't support the 'availability-zone' filter.

The describe_snapshots() method does support a 'volume-size' filter that you can use to filter snapshots by the size of the volume that was used to create them. However, in most cases, you would want to check all snapshots for encryption, regardless of their associated volume's size or location.





User
Response
{
  "errorMessage": "An error occurred (InvalidParameterValue) when calling the DescribeSnapshots operation: The filter 'availability-zone' is invalid",
  "errorType": "ClientError",
  "requestId": "69761397-ae9d-4d48-aebb-8844576374de",
  "stackTrace": [
    "  File \"/var/task/lambda_function.py\", line 32, in lambda_handler\n    response_snapshots = ec2.describe_snapshots(OwnerIds=['self'], Filters=[{'Name': 'availability-zone', 'Values': [availability_zone]}])\n",
    "  File \"/var/runtime/botocore/client.py\", line 530, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n",
    "  File \"/var/runtime/botocore/client.py\", line 960, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n"
  ]
}
ChatGPT
My apologies for the confusion. I made a mistake in my previous response. As I mentioned, the EBS snapshots do not support the availability-zone filter because once a snapshot is created, it can be used to create new volumes in any availability zone within the same region.

Here's the updated function, which doesn't use the availability-zone filter for the describe_snapshots() call:

python
Copy code
import os
import json
import boto3
from datetime import datetime

def lambda_handler(event, context):

    ec2 = boto3.client('ec2')
    s3 = boto3.client('s3')

    bucket_name = os.environ['BUCKET_NAME']
    report_name = os.environ['REPORT_NAME'] + '_' + datetime.now().strftime("%Y%m%d%H%M%S") + '.json'
    availability_zone = os.environ['AVAILABILITY_ZONE']

    unattached_volumes = []
    unencrypted_volumes = []
    unencrypted_snapshots = []

    # Get all volumes in the specified availability zone
    response_volumes = ec2.describe_volumes(Filters=[{'Name': 'availability-zone', 'Values': [availability_zone]}])

    for volume in response_volumes['Volumes']:
        # Check if volume is unattached
        if len(volume['Attachments']) == 0:
            unattached_volumes.append(volume)

        # Check if volume is unencrypted
        if not volume['Encrypted']:
            unencrypted_volumes.append(volume)

    # Get all snapshots
    response_snapshots = ec2.describe_snapshots(OwnerIds=['self'])

    for snapshot in response_snapshots['Snapshots']:
        # Check if snapshot is unencrypted
        if not snapshot['Encrypted']:
            unencrypted_snapshots.append(snapshot)

    # Create a report
    report = {
        'UnattachedVolumes': unattached_volumes,
        'UnencryptedVolumes': unencrypted_volumes,
        'UnencryptedSnapshots': unencrypted_snapshots
    }

    # Write the report to a JSON file
    report_json = json.dumps(report, default=str)

    # Save the report to S3
    s3.put_object(Body=report_json, Bucket=bucket_name, Key=report_name)
This updated code will only filter EBS volumes by availability zone, not EBS snapshots. All non-encrypted EBS snapshots will be included in the report, regardless of their associated volume's location.





User
For snapshot report work well. But no result for volumes.  Check it
ChatGPT
The issue could be that there are no unattached or unencrypted volumes in the specified availability zone. Another possibility is a permission issue if the Lambda function does not have access to the EC2 service.

If the problem persists, you can add some debug logs to the script to get more insights about what's happening. Here's how you might do it:

python
Copy code
import os
import json
import boto3
from datetime import datetime
import logging

# Initialize logger
logging.basicConfig(level=logging.INFO)

def lambda_handler(event, context):

    ec2 = boto3.client('ec2')
    s3 = boto3.client('s3')

    bucket_name = os.environ['BUCKET_NAME']
    report_name = os.environ['REPORT_NAME'] + '_' + datetime.now().strftime("%Y%m%d%H%M%S") + '.json'
    availability_zone = os.environ['AVAILABILITY_ZONE']

    unattached_volumes = []
    unencrypted_volumes = []
    unencrypted_snapshots = []

    # Get all volumes in the specified availability zone
    response_volumes = ec2.describe_volumes(Filters=[{'Name': 'availability-zone', 'Values': [availability_zone]}])

    logging.info(f"Found {len(response_volumes['Volumes'])} volumes in {availability_zone}")

    for volume in response_volumes['Volumes']:
        # Check if volume is unattached
        if len(volume['Attachments']) == 0:
            unattached_volumes.append(volume)

        # Check if volume is unencrypted
        if not volume['Encrypted']:
            unencrypted_volumes.append(volume)

    logging.info(f"Found {len(unattached_volumes)} unattached volumes and {len(unencrypted_volumes)} unencrypted volumes")

    # Get all snapshots
    response_snapshots = ec2.describe_snapshots(OwnerIds=['self'])

    for snapshot in response_snapshots['Snapshots']:
        # Check if snapshot is unencrypted
        if not snapshot['Encrypted']:
            unencrypted_snapshots.append(snapshot)

    logging.info(f"Found {len(unencrypted_snapshots)} unencrypted snapshots")

    # Create a report
    report = {
        'UnattachedVolumes': unattached_volumes,
        'UnencryptedVolumes': unencrypted_volumes,
        'UnencryptedSnapshots': unencrypted_snapshots
    }

    # Write the report to a JSON file
    report_json = json.dumps(report, default=str)

    # Save the report to S3
    s3.put_object(Body=report_json, Bucket=bucket_name, Key=report_name)

    logging.info(f"Report saved to s3://{bucket_name}/{report_name}")
After updating the function with this code, you should see additional logs in Amazon CloudWatch that give you more information about the volumes and snapshots found by the script. If the logs show that no volumes are found, it suggests that there are no volumes in the specified availability zone.





User
Need to select all volumes in region. Don't use availability-zone. Change it on region
ChatGPT
I see, you want to fetch all volumes in the specified region, not just an availability zone. To set the region for Boto3 client, you can use the region_name parameter while creating the client. Here's the modified function:

python
Copy code
import os
import json
import boto3
from datetime import datetime

def lambda_handler(event, context):

    region_name = os.environ['REGION_NAME']
    ec2 = boto3.client('ec2', region_name=region_name)
    s3 = boto3.client('s3', region_name=region_name)

    bucket_name = os.environ['BUCKET_NAME']
    report_name = os.environ['REPORT_NAME'] + '_' + datetime.now().strftime("%Y%m%d%H%M%S") + '.json'

    unattached_volumes = []
    unencrypted_volumes = []
    unencrypted_snapshots = []

    # Get all volumes in the specified region
    response_volumes = ec2.describe_volumes()

    for volume in response_volumes['Volumes']:
        # Check if volume is unattached
        if len(volume['Attachments']) == 0:
            unattached_volumes.append(volume)

        # Check if volume is unencrypted
        if not volume['Encrypted']:
            unencrypted_volumes.append(volume)

    # Get all snapshots in the specified region
    response_snapshots = ec2.describe_snapshots(OwnerIds=['self'])

    for snapshot in response_snapshots['Snapshots']:
        # Check if snapshot is unencrypted
        if not snapshot['Encrypted']:
            unencrypted_snapshots.append(snapshot)

    # Create a report
    report = {
        'UnattachedVolumes': unattached_volumes,
        'UnencryptedVolumes': unencrypted_volumes,
        'UnencryptedSnapshots': unencrypted_snapshots
    }

    # Write the report to a JSON file
    report_json = json.dumps(report, default=str)

    # Save the report to S3
    s3.put_object(Body=report_json, Bucket=bucket_name, Key=report_name)
Don't forget to add REGION_NAME as an environment variable in your Lambda function configuration, with the value set to your desired region (for example, "us-east-1").





User
Change json format in the report. On the second level need have id of volumes or snapshots.
ChatGPT
Sure, to reformat the JSON report to include the ID of each volume or snapshot as a second-level key, you can adjust the report creation part of the Lambda function as follows:

python
Copy code
import os
import json
import boto3
from datetime import datetime

def lambda_handler(event, context):

    region_name = os.environ['REGION_NAME']
    ec2 = boto3.client('ec2', region_name=region_name)
    s3 = boto3.client('s3', region_name=region_name)

    bucket_name = os.environ['BUCKET_NAME']
    report_name = os.environ['REPORT_NAME'] + '_' + datetime.now().strftime("%Y%m%d%H%M%S") + '.json'

    unattached_volumes = {}
    unencrypted_volumes = {}
    unencrypted_snapshots = {}

    # Get all volumes in the specified region
    response_volumes = ec2.describe_volumes()

    for volume in response_volumes['Volumes']:
        volume_id = volume['VolumeId']

        # Check if volume is unattached
        if len(volume['Attachments']) == 0:
            unattached_volumes[volume_id] = volume

        # Check if volume is unencrypted
        if not volume['Encrypted']:
            unencrypted_volumes[volume_id] = volume

    # Get all snapshots in the specified region
    response_snapshots = ec2.describe_snapshots(OwnerIds=['self'])

    for snapshot in response_snapshots['Snapshots']:
        snapshot_id = snapshot['SnapshotId']

        # Check if snapshot is unencrypted
        if not snapshot['Encrypted']:
            unencrypted_snapshots[snapshot_id] = snapshot

    # Create a report
    report = {
        'UnattachedVolumes': unattached_volumes,
        'UnencryptedVolumes': unencrypted_volumes,
        'UnencryptedSnapshots': unencrypted_snapshots
    }

    # Write the report to a JSON file
    report_json = json.dumps(report, default=str)

    # Save the report to S3
    s3.put_object(Body=report_json, Bucket=bucket_name, Key=report_name)
In this updated version, unattached_volumes, unencrypted_volumes, and unencrypted_snapshots are dictionaries instead of lists. The keys of these dictionaries are the volume or snapshot IDs, and the values are the corresponding volume or snapshot objects. This format will allow you to directly look up a specific volume or snapshot in the report using its ID.





User
Refact last version of the code according to best practice
ChatGPT
Sure, here is the refactored version of the Lambda function according to best practices:

We'll use the AWS SDK's built-in paginator feature to ensure we're getting all volumes and snapshots if they span across multiple pages of API responses.
We'll also refactor the report creation part of the Lambda function to its own function for better readability and maintainability.
Finally, we'll separate the report saving part into another function as well.
Here is the refactored code:

python
Copy code
import os
import json
import boto3
from datetime import datetime

def get_volumes_and_snapshots(ec2):
    unattached_volumes = {}
    unencrypted_volumes = {}
    unencrypted_snapshots = {}

    # Get all volumes
    paginator_volumes = ec2.get_paginator('describe_volumes')
    for page_volumes in paginator_volumes.paginate():
        for volume in page_volumes['Volumes']:
            volume_id = volume['VolumeId']

            # Check if volume is unattached
            if len(volume['Attachments']) == 0:
                unattached_volumes[volume_id] = volume

            # Check if volume is unencrypted
            if not volume['Encrypted']:
                unencrypted_volumes[volume_id] = volume

    # Get all snapshots
    paginator_snapshots = ec2.get_paginator('describe_snapshots')
    for page_snapshots in paginator_snapshots.paginate(OwnerIds=['self']):
        for snapshot in page_snapshots['Snapshots']:
            snapshot_id = snapshot['SnapshotId']

            # Check if snapshot is unencrypted
            if not snapshot['Encrypted']:
                unencrypted_snapshots[snapshot_id] = snapshot

    return unattached_volumes, unencrypted_volumes, unencrypted_snapshots


def save_report_to_s3(report, bucket_name, report_name):
    s3 = boto3.client('s3')
    report_json = json.dumps(report, default=str)
    s3.put_object(Body=report_json, Bucket=bucket_name, Key=report_name)


def lambda_handler(event, context):
    region_name = os.environ['REGION_NAME']
    ec2 = boto3.client('ec2', region_name=region_name)

    bucket_name = os.environ['BUCKET_NAME']
    report_name = os.environ['REPORT_NAME'] + '_' + datetime.now().strftime("%Y%m%d%H%M%S") + '.json'

    unattached_volumes, unencrypted_volumes, unencrypted_snapshots = get_volumes_and_snapshots(ec2)

    # Create a report
    report = {
        'UnattachedVolumes': unattached_volumes,
        'UnencryptedVolumes': unencrypted_volumes,
        'UnencryptedSnapshots': unencrypted_snapshots
    }

    save_report_to_s3(report, bucket_name, report_name)
The refactored code separates the concerns of getting volumes/snapshots, creating the report, and saving the report, making the lambda_handler function more readable. The use of paginators also ensures that you won't miss any volumes or snapshots if there are too many to fit into a single API response.





User
Need to configure a schedule to run the serverless service once a day.
ChatGPT
If you're using AWS Lambda, you can trigger your function on a schedule using Amazon CloudWatch Events.

Here are the steps to schedule your Lambda function to run once per day:

Open the AWS Management Console and navigate to the AWS Lambda service.

Choose the name of the Lambda function that you want to schedule.

In the Designer section, choose Add trigger.

In the trigger configuration, select EventBridge (CloudWatch Events).

Choose Create a new rule.

Enter a rule name and optionally a description.

Under Rule type, choose Schedule expression.

In the Schedule expression box, input rate(1 day). This will schedule your Lambda function to run once every day.

Choose Add to create the trigger and then Save in the upper right corner to apply the changes.

Remember that the schedule expression rate or cron parameters are in UTC by default. So, you will need to adjust the timing accordingly if you need it to run at a specific time in a different timezone. For example, the cron expression cron(0 12 * * ? *) will run your function every day at noon UTC.

If you are using the Serverless Framework for managing your serverless applications, you can set the schedule directly in your serverless.yml configuration file like this:

yaml
Copy code
functions:
  yourFunction:
    handler: path_to_your_function.lambda_handler
    events:
      - schedule: rate(1 day)
After setting this, you can deploy your changes with the serverless deploy command. This will automatically create the necessary CloudWatch Events rule for you.